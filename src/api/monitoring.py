"""
ç›‘æ§å‘Šè­¦æ¨¡å—

å®ç°æœåŠ¡ç›‘æ§å’Œå‘Šè­¦åŠŸèƒ½
"""

import time
import psutil
import torch
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field
from datetime import datetime
import logging
import json
from pathlib import Path
from collections import deque

logger = logging.getLogger(__name__)


@dataclass
class Alert:
    """å‘Šè­¦ä¿¡æ¯"""
    level: str  # info/warning/error/critical
    message: str
    timestamp: datetime = field(default_factory=datetime.now)
    metric_name: str = ""
    metric_value: float = 0.0
    threshold: float = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "level": self.level,
            "message": self.message,
            "timestamp": self.timestamp.isoformat(),
            "metric_name": self.metric_name,
            "metric_value": self.metric_value,
            "threshold": self.threshold
        }


@dataclass
class MonitoringConfig:
    """ç›‘æ§é…ç½®"""
    # æ€§èƒ½é˜ˆå€¼
    max_latency_ms: float = 100.0
    max_memory_percent: float = 80.0
    max_cpu_percent: float = 80.0
    max_gpu_memory_percent: float = 90.0
    
    # é”™è¯¯ç‡é˜ˆå€¼
    max_error_rate: float = 0.05
    
    # ç›‘æ§çª—å£
    window_size: int = 1000
    
    # å‘Šè­¦å†·å´æœŸï¼ˆç§’ï¼‰
    alert_cooldown: int = 300


class SystemMonitor:
    """ç³»ç»Ÿç›‘æ§å™¨"""
    
    def __init__(self, config: MonitoringConfig):
        """
        åˆå§‹åŒ–ç³»ç»Ÿç›‘æ§å™¨
        
        Args:
            config: ç›‘æ§é…ç½®
        """
        self.config = config
        self.alerts: deque = deque(maxlen=1000)
        self.last_alert_time: Dict[str, float] = {}
        
        # æ€§èƒ½æŒ‡æ ‡
        self.latencies: deque = deque(maxlen=config.window_size)
        self.error_count = 0
        self.total_requests = 0
        
        logger.info("System monitor initialized")
    
    def record_request(self, latency: float, success: bool = True):
        """
        è®°å½•è¯·æ±‚
        
        Args:
            latency: å»¶è¿Ÿï¼ˆç§’ï¼‰
            success: æ˜¯å¦æˆåŠŸ
        """
        self.latencies.append(latency)
        self.total_requests += 1
        
        if not success:
            self.error_count += 1
    
    def check_latency(self) -> Optional[Alert]:
        """æ£€æŸ¥å»¶è¿Ÿ"""
        if not self.latencies:
            return None
        
        avg_latency_ms = sum(self.latencies) / len(self.latencies) * 1000
        
        if avg_latency_ms > self.config.max_latency_ms:
            return Alert(
                level="warning",
                message=f"High latency detected: {avg_latency_ms:.2f}ms",
                metric_name="latency",
                metric_value=avg_latency_ms,
                threshold=self.config.max_latency_ms
            )
        
        return None
    
    def check_error_rate(self) -> Optional[Alert]:
        """æ£€æŸ¥é”™è¯¯ç‡"""
        if self.total_requests == 0:
            return None
        
        error_rate = self.error_count / self.total_requests
        
        if error_rate > self.config.max_error_rate:
            return Alert(
                level="error",
                message=f"High error rate: {error_rate:.2%}",
                metric_name="error_rate",
                metric_value=error_rate,
                threshold=self.config.max_error_rate
            )
        
        return None
    
    def check_memory(self) -> Optional[Alert]:
        """æ£€æŸ¥å†…å­˜ä½¿ç”¨"""
        memory = psutil.virtual_memory()
        memory_percent = memory.percent
        
        if memory_percent > self.config.max_memory_percent:
            return Alert(
                level="warning",
                message=f"High memory usage: {memory_percent:.1f}%",
                metric_name="memory",
                metric_value=memory_percent,
                threshold=self.config.max_memory_percent
            )
        
        return None
    
    def check_cpu(self) -> Optional[Alert]:
        """æ£€æŸ¥CPUä½¿ç”¨"""
        cpu_percent = psutil.cpu_percent(interval=1)
        
        if cpu_percent > self.config.max_cpu_percent:
            return Alert(
                level="warning",
                message=f"High CPU usage: {cpu_percent:.1f}%",
                metric_name="cpu",
                metric_value=cpu_percent,
                threshold=self.config.max_cpu_percent
            )
        
        return None
    
    def check_gpu(self) -> Optional[Alert]:
        """æ£€æŸ¥GPUä½¿ç”¨"""
        if not torch.cuda.is_available():
            return None
        
        try:
            gpu_memory_allocated = torch.cuda.memory_allocated() / torch.cuda.max_memory_allocated() * 100
            
            if gpu_memory_allocated > self.config.max_gpu_memory_percent:
                return Alert(
                    level="warning",
                    message=f"High GPU memory usage: {gpu_memory_allocated:.1f}%",
                    metric_name="gpu_memory",
                    metric_value=gpu_memory_allocated,
                    threshold=self.config.max_gpu_memory_percent
                )
        except:
            pass
        
        return None
    
    def check_all(self) -> List[Alert]:
        """æ‰§è¡Œæ‰€æœ‰æ£€æŸ¥"""
        alerts = []
        
        # æ£€æŸ¥å„é¡¹æŒ‡æ ‡
        checks = [
            self.check_latency(),
            self.check_error_rate(),
            self.check_memory(),
            self.check_cpu(),
            self.check_gpu()
        ]
        
        for alert in checks:
            if alert is not None:
                # æ£€æŸ¥å†·å´æœŸ
                if self._should_alert(alert.metric_name):
                    alerts.append(alert)
                    self.alerts.append(alert)
                    self.last_alert_time[alert.metric_name] = time.time()
                    logger.warning(f"Alert: {alert.message}")
        
        return alerts
    
    def _should_alert(self, metric_name: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥å‘é€å‘Šè­¦ï¼ˆè€ƒè™‘å†·å´æœŸï¼‰"""
        if metric_name not in self.last_alert_time:
            return True
        
        elapsed = time.time() - self.last_alert_time[metric_name]
        return elapsed > self.config.alert_cooldown
    
    def get_metrics(self) -> Dict[str, Any]:
        """è·å–ç›‘æ§æŒ‡æ ‡"""
        metrics = {
            "timestamp": datetime.now().isoformat(),
            "total_requests": self.total_requests,
            "error_count": self.error_count,
            "error_rate": self.error_count / max(self.total_requests, 1),
            "system": {
                "cpu_percent": psutil.cpu_percent(),
                "memory_percent": psutil.virtual_memory().percent,
                "disk_percent": psutil.disk_usage('/').percent
            }
        }
        
        # å»¶è¿Ÿç»Ÿè®¡
        if self.latencies:
            import numpy as np
            latencies_ms = [l * 1000 for l in self.latencies]
            metrics["latency"] = {
                "avg_ms": np.mean(latencies_ms),
                "p50_ms": np.percentile(latencies_ms, 50),
                "p95_ms": np.percentile(latencies_ms, 95),
                "p99_ms": np.percentile(latencies_ms, 99),
                "max_ms": np.max(latencies_ms)
            }
        
        # GPUä¿¡æ¯
        if torch.cuda.is_available():
            metrics["gpu"] = {
                "available": True,
                "device_count": torch.cuda.device_count(),
                "current_device": torch.cuda.current_device(),
                "memory_allocated_mb": torch.cuda.memory_allocated() / 1024**2,
                "memory_reserved_mb": torch.cuda.memory_reserved() / 1024**2
            }
        else:
            metrics["gpu"] = {"available": False}
        
        return metrics
    
    def get_alerts(self, level: Optional[str] = None) -> List[Dict[str, Any]]:
        """
        è·å–å‘Šè­¦åˆ—è¡¨
        
        Args:
            level: å‘Šè­¦çº§åˆ«è¿‡æ»¤
            
        Returns:
            alerts: å‘Šè­¦åˆ—è¡¨
        """
        alerts = [alert.to_dict() for alert in self.alerts]
        
        if level:
            alerts = [a for a in alerts if a["level"] == level]
        
        return alerts
    
    def export_metrics(self, output_path: str):
        """
        å¯¼å‡ºç›‘æ§æŒ‡æ ‡
        
        Args:
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        metrics = self.get_metrics()
        
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w') as f:
            json.dump(metrics, f, indent=2)
        
        logger.info(f"Metrics exported to {output_path}")


class PerformanceDashboard:
    """æ€§èƒ½ä»ªè¡¨æ¿"""
    
    def __init__(self, monitor: SystemMonitor):
        """
        åˆå§‹åŒ–ä»ªè¡¨æ¿
        
        Args:
            monitor: ç³»ç»Ÿç›‘æ§å™¨
        """
        self.monitor = monitor
    
    def print_dashboard(self):
        """æ‰“å°ä»ªè¡¨æ¿"""
        metrics = self.monitor.get_metrics()
        
        print("\n" + "=" * 60)
        print("PERFORMANCE DASHBOARD")
        print("=" * 60)
        
        # è¯·æ±‚ç»Ÿè®¡
        print(f"\nğŸ“Š Request Statistics:")
        print(f"  Total Requests: {metrics['total_requests']}")
        print(f"  Error Count: {metrics['error_count']}")
        print(f"  Error Rate: {metrics['error_rate']:.2%}")
        
        # å»¶è¿Ÿç»Ÿè®¡
        if "latency" in metrics:
            print(f"\nâ±ï¸  Latency:")
            print(f"  Average: {metrics['latency']['avg_ms']:.2f}ms")
            print(f"  P95: {metrics['latency']['p95_ms']:.2f}ms")
            print(f"  P99: {metrics['latency']['p99_ms']:.2f}ms")
            print(f"  Max: {metrics['latency']['max_ms']:.2f}ms")
        
        # ç³»ç»Ÿèµ„æº
        print(f"\nğŸ’» System Resources:")
        print(f"  CPU: {metrics['system']['cpu_percent']:.1f}%")
        print(f"  Memory: {metrics['system']['memory_percent']:.1f}%")
        print(f"  Disk: {metrics['system']['disk_percent']:.1f}%")
        
        # GPUä¿¡æ¯
        if metrics['gpu']['available']:
            print(f"\nğŸ® GPU:")
            print(f"  Device Count: {metrics['gpu']['device_count']}")
            print(f"  Memory Allocated: {metrics['gpu']['memory_allocated_mb']:.1f}MB")
            print(f"  Memory Reserved: {metrics['gpu']['memory_reserved_mb']:.1f}MB")
        
        # å‘Šè­¦
        alerts = self.monitor.get_alerts()
        if alerts:
            print(f"\nâš ï¸  Recent Alerts ({len(alerts)}):")
            for alert in alerts[-5:]:  # æ˜¾ç¤ºæœ€è¿‘5æ¡
                print(f"  [{alert['level'].upper()}] {alert['message']}")
        
        print("\n" + "=" * 60)


def example_usage():
    """ä½¿ç”¨ç¤ºä¾‹"""
    # åˆ›å»ºç›‘æ§å™¨
    config = MonitoringConfig(
        max_latency_ms=100.0,
        max_memory_percent=80.0,
        max_error_rate=0.05
    )
    monitor = SystemMonitor(config)
    
    # æ¨¡æ‹Ÿè¯·æ±‚
    import random
    for i in range(100):
        latency = random.uniform(0.01, 0.15)
        success = random.random() > 0.02
        monitor.record_request(latency, success)
    
    # æ£€æŸ¥å‘Šè­¦
    alerts = monitor.check_all()
    print(f"Alerts generated: {len(alerts)}")
    
    # æ˜¾ç¤ºä»ªè¡¨æ¿
    dashboard = PerformanceDashboard(monitor)
    dashboard.print_dashboard()
    
    # å¯¼å‡ºæŒ‡æ ‡
    monitor.export_metrics("logs/monitoring/metrics.json")


if __name__ == "__main__":
    example_usage()