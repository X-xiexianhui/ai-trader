# PPO强化学习配置文件
# 用于配置PPO训练的所有超参数和环境设置

# 模型配置
model:
  state_dim: 263  # 状态维度：256(Transformer) + 4(持仓信息) + 3(风险参数)
  hidden_dim: 512  # 隐藏层维度
  dropout: 0.1  # Dropout率
  share_features: false  # 是否共享Actor-Critic特征层

# 环境配置
environment:
  initial_balance: 100000.0  # 初始资金
  max_position_size: 0.5  # 最大仓位比例
  commission_rate: 0.0003  # 手续费率（0.03%）
  slippage: 0.0001  # 滑点（0.01%）
  max_steps: 1000  # 每个episode最大步数
  
  # 奖励函数权重
  reward_weights:
    profit: 1.0  # 盈利奖励权重
    risk_penalty: 0.3  # 风险惩罚权重
    stability: 0.2  # 稳定性奖励权重
    drawdown_penalty: 0.5  # 回撤惩罚权重
    holding_penalty: 0.01  # 持仓时间惩罚权重

# PPO训练配置
training:
  # 基础参数
  total_timesteps: 1000000  # 总训练步数
  n_steps: 2048  # 每次rollout的步数
  n_epochs: 10  # 每次更新的epoch数
  mini_batch_size: 256  # 小批量大小
  
  # PPO算法参数
  gamma: 0.99  # 折扣因子
  gae_lambda: 0.95  # GAE lambda参数
  clip_range: 0.2  # PPO clip范围
  clip_range_vf: null  # 价值函数clip范围（null表示不使用）
  ent_coef: 0.01  # 熵系数
  vf_coef: 0.5  # 价值函数损失系数
  max_grad_norm: 0.5  # 梯度裁剪阈值
  target_kl: 0.01  # 目标KL散度（用于早停）
  
  # 优化器参数
  policy_lr: 0.0001  # 策略网络学习率
  value_lr: 0.0003  # 价值网络学习率
  use_lr_scheduler: true  # 是否使用学习率调度器
  lr_step_size: 100  # 学习率衰减步长
  lr_gamma: 0.95  # 学习率衰减因子
  
  # 训练控制
  log_interval: 10  # 日志记录间隔（更新次数）
  save_interval: 100  # 模型保存间隔（更新次数）
  eval_interval: 50  # 评估间隔（更新次数）
  
  # 设备配置
  device: 'cuda'  # 训练设备：'cuda' 或 'cpu'
  num_workers: 4  # 数据加载线程数

# 评估配置
evaluation:
  num_episodes: 20  # 评估episode数量
  deterministic: true  # 是否使用确定性策略
  render: false  # 是否渲染环境
  save_results: true  # 是否保存评估结果

# 数据配置
data:
  # Transformer模型路径（用于生成状态向量）
  transformer_model_path: 'models/transformer/best_model.pt'
  
  # 数据路径
  train_data_path: 'data/processed/train_features.pkl'
  val_data_path: 'data/processed/val_features.pkl'
  test_data_path: 'data/processed/test_features.pkl'
  
  # 数据预处理
  normalize: true  # 是否归一化
  sequence_length: 60  # 序列长度（用于Transformer）

# 保存配置
save:
  model_dir: 'models/ppo'  # 模型保存目录
  log_dir: 'logs/ppo'  # 日志保存目录
  checkpoint_prefix: 'ppo_checkpoint'  # 检查点文件前缀
  save_best_only: false  # 是否只保存最佳模型

# 日志配置
logging:
  level: 'INFO'  # 日志级别：DEBUG, INFO, WARNING, ERROR
  format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
  file: 'logs/ppo/training.log'  # 日志文件路径
  console: true  # 是否输出到控制台

# 实验跟踪配置
experiment:
  name: 'ppo_trading'  # 实验名称
  tags: ['ppo', 'trading', 'reinforcement_learning']  # 实验标签
  notes: 'PPO强化学习交易策略训练'  # 实验备注
  
  # Weights & Biases配置（可选）
  use_wandb: false  # 是否使用W&B
  wandb_project: 'ai-trader'  # W&B项目名称
  wandb_entity: null  # W&B实体名称

# 高级配置
advanced:
  # 经验回放
  use_prioritized_replay: false  # 是否使用优先经验回放
  
  # 多环境并行
  num_envs: 1  # 并行环境数量
  
  # 探索策略
  exploration_noise: 0.1  # 探索噪声
  
  # 正则化
  l2_reg: 0.0001  # L2正则化系数
  
  # 早停
  use_early_stopping: true  # 是否使用早停
  patience: 50  # 早停耐心值
  min_delta: 0.001  # 最小改善阈值

# 调试配置
debug:
  enabled: false  # 是否启用调试模式
  check_numerics: true  # 是否检查数值稳定性
  profile: false  # 是否启用性能分析
  verbose: false  # 是否输出详细信息